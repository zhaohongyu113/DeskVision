# 配置调整成uv
让ai把现在的安装配置 调整成uv形式的
# uv安装依赖和环境
1) 创建虚拟环境（uv venv）
创建 venv（建议 Python 3.10，和项目 README 一致）：
uv venv --python 3.10
激活：
.\.venv\Scripts\activate

**关键检查：确认你用的就是 `.venv` 里的 Python（避免装错环境）**

```powershell
python -c "import sys; print(sys.executable)"
```

输出里应该包含 `...\\DeskVision\\.venv\\Scripts\\python.exe`。

> 如果你不想/不方便激活 venv，也可以始终用 `uv run` 或给 `uv pip` 指定 `--python`（下面会给例子）。
2) 安装 PyTorch（CUDA 版，单独装）
CUDA 12.1：
```powershell
# 推荐：显式指定安装到本项目的 .venv（不依赖是否已激活）
uv pip install --python .\.venv\Scripts\python.exe torch==2.1.2 torchvision==0.16.2 --index-url https://download.pytorch.org/whl/cu121
```

如果你已经激活了 venv，也可以直接：

```powershell
uv pip install torch==2.1.2 torchvision==0.16.2 --index-url https://download.pytorch.org/whl/cu121
```

（可选）如果你想复用下载缓存/离线安装：

```powershell
# 一次下载，多环境复用
mkdir wheelhouse
uv pip download torch==2.1.2 torchvision==0.16.2 --index-url https://download.pytorch.org/whl/cu121 -d .\wheelhouse

# 新环境里离线安装
uv pip install --python .\.venv\Scripts\python.exe --no-index --find-links .\wheelhouse torch==2.1.2 torchvision==0.16.2
```
验证 GPU：

推荐用 `uv run`，不怕你没激活 venv：

```powershell
uv run python -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'no cuda')"
```

如果你看到 `ModuleNotFoundError: No module named 'torch'`，通常是你执行的 `python` 不是 `.venv` 里的：

```powershell
python -c "import sys; print(sys.executable)"
```

然后用下面这条把 torch 安装进正确的 venv：

```powershell
uv pip install --python .\.venv\Scripts\python.exe torch==2.1.2 torchvision==0.16.2 --index-url https://download.pytorch.org/whl/cu121
```

3) 安装推理 + Gradio 的最小依赖

在项目根目录执行：

```powershell
uv pip install --python .\.venv\Scripts\python.exe -r requirements-infer-win-uv.txt
uv pip install --python .\.venv\Scripts\python.exe -e .
```

验证导入（推荐用 `uv run`，不依赖你是否激活 venv）：

```powershell
uv run python -c "import llava; print('llava import ok')"
```

**重要说明**：推理/Gradio 不要安装 `llava[train]`（例如 `uv pip install -e ".[train]"`）。
因为它会拉 `deepspeed==0.14.4` 等训练依赖，Windows 上通常会安装失败；而推理并不需要这些包。

4) 下载模型权重（必须）
官方说从 HuggingFace 下载 GUIExplorer 权重，并放到 ./pretrained_models。

**模型路径如何配置（不想写死 `pretrained_models`）**

推理脚本会按优先级读取模型目录：

1) 环境变量 `DESKVISION_PRETRAINED_MODELS`
2) [scripts/inference/config.json](scripts/inference/config.json) 里的 `pretrained_models`
3) 默认 `../../pretrained_models`

你可以复制示例文件：

```powershell
copy scripts\inference\config.example.json scripts\inference\config.json
```

然后编辑 `scripts/inference/config.json`：

```json
{ "pretrained_models": "../../pretrained_models" }
```

推荐命令行下载（更不容易漏文件）：

uv pip install -U "huggingface_hub[cli]"
需要登录时：huggingface-cli login
下载到项目根目录的 pretrained_models：
huggingface-cli download caca9527/GUIExplorer --local-dir pretrained_models --local-dir-use-symlinks False
关键检查：pretrained_models 里应该能看到 config.json 等模型文件。
如果你下载后变成了 pretrained_models\某个子目录\config.json，那你运行时把 --pretrained_models 指到那个子目录即可。

5) 跑命令行推理（infer.py）
必须在 inference 目录运行（因为字体 default.ttf 用了相对路径）：

cd scripts\inference
Grounding 示例：

python infer.py --task grounding --input_text "开始学习" --input_image .\test_imgs\00060.png
Instruction 示例：

python infer.py --task instruction --input_text "我想打开桌面" --input_image .\test_imgs\pw_1.png
如果模型不在默认目录（默认是 ../../pretrained_models），加参数：

python infer.py --task grounding --input_text "开始学习" --input_image .\test_imgs\00060.png --pretrained_models ..\..\pretrained_models\<你的模型目录>
输出图会在：

scripts\inference\visual_results\
6) 跑 Gradio Demo（demo.py）
同样在 inference：

python demo.py
浏览器打开 http://127.0.0.1:9527（或终端打印的地址）。